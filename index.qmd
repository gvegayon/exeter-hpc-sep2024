---
title: A Brief Introduction to High-Performance Computing
subtitle: With Applications in R
author: George G. Vega Yon, Ph.D.
institute: University of Utah, EEUU
date: 2024-08-23
format:
  revealjs:
    slide-number: true
    code-copy: true
    smaller: true
    fig-format: svg
    fig-dpi: 200
    fig-asp: 1
bibliography: references.bib
embed-resources: true
---

# Fundamentals {background-color="#515A5A"}

![](https://raw.githubusercontent.com/USCbiostats/hpc-with-r/df60e1cfdc0f848f4f0de5a0aa7d0833f4cfe3d5/fig/bulldog-teaches-baby-crawl.gif){fig-align="center"}



## High-Performance Computing: An overview 

Loosely, from R's perspective, we can think of HPC in terms of two, maybe three things:

1.  Big data: How to work with data that doesn't fit your computer

2.  Parallel computing: How to take advantage of multiple core systems

3.  Compiled code: Write your own low-level code (if R doesn't has it yet...)

(Checkout [CRAN Task View on HPC](https://cran.r-project.org/web/views/HighPerformanceComputing.html))


## Some vocabulary for HPC 


::: {.columns layout-align="center"}
::: {.column width="33%"}
High Throughput Computing Cluster (HTC Cluster)

![Open Science Grid Consortium (OSG) <https://osg-htc.org>](fig/OSG_Map.png){width=80%}
:::
::: {.column width="33%"}
Supercomputer (HPC Cluster)

![The Exascale-class HPE Cray EX Supercomputer at Oak Ridge National Laboratory (fastest as of June 2024)](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Frontier_Supercomputer_%282%29.jpg/585px-Frontier_Supercomputer_%282%29.jpg){width=80%}
:::
::: {.column width="33%"}
Single Instruction, Multiple Data (SIMD)

![Vadikus, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/SIMD2.svg/466px-SIMD2.svg.png){width=80%}
:::
:::

**In terms of scale**

::: {layout-align="center"}
HTC > HPC > Single node > Socket > Core > Thread | SIMD vectorization
:::

## What's "a core"? 

![Source: Original figure from LUMI consortium documentation [@lumi2023]](https://raw.githubusercontent.com/gvegayon/appliedhpcr/e33949f25184bed5e84b02c356656782ddd5a46b/fig/socket-core-threads.svg){width=90% height=400px}

How many cores does your computer has?

```{r}
#| label: 03-how-many-cores
#| echo: true
parallel::detectCores()
```

## What is parallel computing? 

```r
f <- function(n) n*2

f(1:4)
```

::: {.columns}
::: {.column width="40%"}
- Using a single core.
- One element at a time.
- 3 idle cores.
:::
::: {.column width="60%"}
![](fig/pll-computing-explained-serial.svg)
:::
:::


## What is parallel computing? 

```r
f <- function(n) n*2
f_pll <- ...magic to parallelize f...
f_pll(1:4)
```

::: {.columns}
::: {.column width="40%"}
- Using 4 cores.
- 4 elements at a time.
- No idle cores.
- 4 times faster.
:::
::: {.column width="60%"}
![](fig/pll-computing-explained-parallel.svg)
:::
:::


## {background-color="#515A5A" style="margin:auto;text-align:center;"}

<text style="color:white;">Let's think before we start...</text>

![](https://media.giphy.com/media/Dwclsfe6Gb91m/giphy.gif){style="width:500px"}

<text style="color:white;">When is it a good idea to go HPC?</text>

## When is it a good idea? 

![Ask yourself these questions before jumping into HPC!](fig/when_to_parallel.svg){fig-align="center" fig-alt="Ask yourself these questions before jumping into HPC!" width="70%"}


## When is it a good idea?

Things that are easily parallelizable:

- Bootstrapping.
- Cross-validation.
- Monte Carlo simulations.
- Multiple MCMC chains.

Things that are not easily parallelizable:

- Regression models.
- Within Chain MCMC.
- Matrix Algebra (generally)[^whenmatrix].
- Any sequential algorithm.

[^whenmatrix]: Parallelization of matrix operations is usually done via SIMD instructions, like those in GPUs and some CPUs. Most cases it applies via implicit parallelism (not at the user level).

## Overhead cost

::: {style="font-size:80%"}
- Parallelization is not free: Most cost is in sending+receiving data.

- In R (and other flavors), you can mitigate by (i) reducing the amount of data communicated, and (ii) reducing the number of times you communicate.
:::

```{r}
#| label: overhead-cost
#| cache: true
#| echo: false
# Simulating data for the 
nvars <- c(100, 500, 1000) * 3
n     <- 10000
set.seed(331)
x <- matrix(rnorm(n*max(nvars)), nrow=n)
y <- rnorm(n)

# Saving for later
dat_overhead <- list(x = x, y = y, nvars = nvars)

# Creating the cluster object
library(parallel)
cl <- makePSOCKcluster(4)
clusterExport(cl, c("x", "y", "nvars"))

# Making room
timing_serial_lm_all <- list()
timing_parallel_lm_all <- list()
timing_serial_lm <- list()
timing_parallel_lm <- list()
timing_parallel_lm_lite <- list()

# Runs the call and returns the time
timer <- function(x., fun, cl = NULL) {
  if (is.null(cl)) {
    system.time({
      apply(x., 2, fun, y = y)
    }) * 1000
  } else {
    system.time({
      parApply(cl, x., 2, fun, y = y)
    }) * 1000
  }
}

for (i in seq_along(nvars)) {

  timing_serial_lm_all[[i]] <- timer(x[, 1:nvars[i]], function(x., y) lm(y ~ x.))
  timing_parallel_lm_all[[i]] <- timer(x[, 1:nvars[i]], function(x., y) lm(y ~ x.), cl)

  timing_serial_lm[[i]] <- timer(x[, 1:nvars[i]], function(x., y) coef(lm(y ~ x.)))
  timing_parallel_lm[[i]] <- timer(x[, 1:nvars[i]], function(x., y) coef(lm(y ~ x.)), cl)

  timing_parallel_lm_lite[[i]] <-  system.time({
    mclapply(1:nvars[i], function(j) coef(lm(y ~ x[,j])), mc.cores = 4)
  }) * 1000

}
stopCluster(cl)

# Preparing the data
timing_serial_lm<- do.call(rbind, timing_serial_lm) |> data.frame()
timing_parallel_lm<- do.call(rbind, timing_parallel_lm) |> data.frame()

timing_serial_lm_all<- do.call(rbind, timing_serial_lm_all) |> data.frame()
timing_parallel_lm_all<- do.call(rbind, timing_parallel_lm_all) |> data.frame()

timing_parallel_lm_lite<- do.call(rbind, timing_parallel_lm_lite) |> data.frame()
```

```{r}
#| echo: false
#| fig-asp: .6
#| fig-align: center
#| fig-cap: "Overhead cost of parallelization: Fitting $y = \\alpha + \\beta_k X_k + \\varepsilon,\\quad k = 1, \\dots$ (more about this later)"
# Nice color palette
palette(hcl.colors(8, "viridis"))

# Visualize
op <- par(cex=1.5, mai = par("mai") * c(1.5,1.5,0,0))
maxy <- range(c(timing_serial_lm_all[, "elapsed"], timing_parallel_lm_all[, "elapsed"], timing_parallel_lm[, "elapsed"], timing_parallel_lm_lite[, "elapsed"]))

plot(timing_serial_lm_all[, "elapsed"] ~ nvars, type="b", col=1, pch=19, xlab="Number of variables", ylab="Time (ms) (log-scale)", lty=1, ylim=maxy, log="y", lwd=4)
lines(timing_parallel_lm_all[, "elapsed"] ~ nvars, type="b", col=2, pch=19, lty=2, lwd = 4)
lines(timing_parallel_lm[, "elapsed"] ~ nvars, type="b", col=3, pch=19, lty=3, lwd = 4)
lines(timing_parallel_lm_lite[, "elapsed"] ~ nvars, type="b", col=4, pch=19, lty=4, lwd = 4)

legend("bottomright", legend=c("Serial", "Naive parallel", "Good parallel", "Best parallel"), col=1:4, lty=1:4, bty="n", lwd=4, cex=1.2)
par(op)
```


# Parallel computing in R {background-color="#515A5A"}

![](https://raw.githubusercontent.com/USCbiostats/hpc-with-r/master/fig/spider-toddler.gif){fig-align="center" fig-alt="Spiderman teaching a toddler to walk"}

## Parallel computing in R

While there are several alternatives (just take a look at the
[High-Performance Computing Task View](https://cran.r-project.org/web/views/HighPerformanceComputing.html)),
we'll focus on the following R-packages for **explicit parallelism**

Some examples:

> *   [**parallel**](https://cran.r-project.org/package=parallel): R package that provides '[s]upport for parallel computation,
    including random-number generation'.

> *   [**foreach**](https://cran.r-project.org/package=foreach): R package for 'general iteration over elements' in parallel fashion.

> *   [**future**](https://cran.r-project.org/package=future): '[A] lightweight and
    unified Future API for sequential and parallel processing of R
    expression via futures.'

> *   [**slurmR**](https://cran.r-project.org/package=slurmR): R package for working with
    the Slurm Workload Manager (by yours truly).
    
Implicit parallelism, on the other hand, are out-of-the-box tools that allow the
programmer not to worry about parallelization, e.g. such as
[**gpuR**](https://cran.r-project.org/package=gpuR) for Matrix manipulation using
GPU, [**tensorflow**](https://cran.r-project.org/package=tensorflow)

---

And there's also a more advanced set of options

> *   [**Rcpp**](https://cran.r-project.org/package=Rcpp) + [OpenMP](https://www.openmp.org):
    [Rcpp](https://cran.r-project.org/package=Rcpp) is an R package for integrating
    R with C++, and OpenMP is a library for high-level parallelism for C/C++ and
    Fortran.

> *   A ton of other type of resources, notably the tools for working with 
    batch schedulers such as Slurm, HTCondor, etc.
    
## The parallel package

::: {.columns}
::: {.column width="50%"}
- Explicit parallelism.
- Parallel computing as multiple R sessions.
- Clusters can be made of both local and remote sessions
- Multiple types of cluster: `PSOCK`, `Fork`, `MPI`, etc.
:::
::: {.column width="50%"}
![](fig/parallel-package.svg)
:::
:::

## Parallel workflow

(Usually) We do the following:

::: {.fragment}
1.  Create a `PSOCK/FORK` (or other) cluster using `makePSOCKCluster`/`makeForkCluster`
    (or `makeCluster`)
:::

::: {.fragment}
2.  Copy/prepare each R session (if you are using a `PSOCK` cluster):

    a.  Copy objects with `clusterExport`

    b.  Pass expressions with `clusterEvalQ`

    c.  Set a seed
:::

::: {.fragment}
3.  Do your call: `parApply`, `parLapply`, etc. 
:::

::: {.fragment}
4.  Stop the cluster with `clusterStop`
:::

## Types of clusters


| Type | Description | Pros | Cons | 
|------|-------------|------|------|
| `PSOCK` | Multiple machines via [socket](https://en.wikipedia.org/w/index.php?title=Network_socket&oldid=1234852476) connection | Works in all OSs | Slowest |
| `FORK` | Single machine via [forking](https://en.wikipedia.org/w/index.php?title=Fork_(system_call)&oldid=1241385768) | Avoids memory duplication | Only for Unix-based |
| `MPI`[^mpi] | Multiple machines via [Message Passage Interface](https://en.wikipedia.org/w/index.php?title=Message_Passing_Interface&oldid=1238011288) | Best alternative for HPC clusters | Sometimes hard to setup |

Using PSOCK, the [`slurmR`](https://cran.r-project.org/package=slurmR){target="_blank"} package creates clusters featuring multiple nodes in HPC environments, think *hundreds of cores*.

[^mpi]: Requires the [`Rmpi`](https://cran.r-project.org/package=Rmpi) package

# Hands-on {background-color="#515A5A"}

![](https://i.imgflip.com/38jiku.jpg){fig-align="center" fig-alt="Emergency broadcat: Your R code will get some seriuos speed boost" width="50%"}

## Ex 1: Hello world!

```{r}
#| label: parallel-ex-psock
#| echo: true
# 1. CREATING A CLUSTER
library(parallel)
cl <- makePSOCKcluster(4)    
x  <- 20

# 2. PREPARING THE CLUSTER
clusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`
clusterExport(cl, "x")

# 3. DO YOUR CALL
clusterEvalQ(cl, {
  paste0("Hello from process #", Sys.getpid(), ". x = ", x)
})

# 4. STOP THE CLUSTER
stopCluster(cl)
```


## Ex 2: Regressions

**Problem**: Run multiple regressions on a very wide dataset. We need to fit the
following model:

$$
y = X_i\beta_i + \varepsilon,\quad \varepsilon\sim N(0, \sigma^2_i),\quad\forall i
$$

```{r, echo=FALSE}
#| label: lots-of-lm-dgp
set.seed(131)
y <- rnorm(500)
X <- matrix(rnorm(500*999), nrow = 500, dimnames = list(1:500, sprintf("x%03d", 1:999)))
```

```{r}
#| label: lots-of-lm-dim
#| echo: true
dim(X)
X[1:6, 1:5]
str(y)
```

## Ex 2: Regressions - Serial

```{r}
#| label: lots-of-lm-serial
#| echo: true
#| strip-white: false
 
 
ans <- apply(
  
  X      = X,
  MARGIN = 2,
  FUN    = function(x, y) coef(lm(y ~ x)),
  y      = y
  )

ans[,1:5]
```

## Ex 2: Regressions - Parallel

```{r}
#| label: lots-of-lm-parallel
#| echo: true
cl <- parallel::makePSOCKcluster(4L)
ans <- parallel::parApply(
  cl     = cl,
  X      = X,
  MARGIN = 2,
  FUN    = function(x, y) coef(lm(y ~ x)),
  y      = y
  )

ans[,1:5]
```

-----

Are we going any faster?

```{r}
#| label: lots-of-lm-bench
#| echo: true
library(microbenchmark)
microbenchmark(
  parallel = parallel::parApply(
    cl  = cl,
    X   = X, MARGIN = 2,
    FUN = function(x, y) coef(lm(y ~ x)),
    y   = y
    ),
  serial = apply(
    X   = X, MARGIN = 2,
    FUN = function(x, y) coef(lm(y ~ x)),
    y   = y
    ),
    times = 10,
    unit = "relative"
)
parallel::stopCluster(cl)
```

## Ex 3: Bootstrap

Problem: We want to bootstrap a logistic regression model. We need to fit the
following model:

$$
P(Y=1) = \text{logit}^{-1}\left(X\beta\right)
$$


```{r}
#| label: parallel-ex-bootstrap
#| echo: false
# Simulating some data
n <- 100
k <- 5
set.seed(33)
X <- matrix(rnorm(n*k), ncol=k)
b <- cbind(rnorm(k))
y <- rbinom(n, 1, prob=plogis(2 + X %*% b))
# glm(y ~ X, family = binomial("logit")) |> coef()
```

```{r}
#| label: parallel-ex-bootstrap-data
#| echo: true
dim(X)
head(X)
y[1:6]
```

## Ex 3: Bootstrap - Serial

```{r}
#| label: parallel-ex-bootstrap-serial
#| echo: true
my_boot <- function(y, X, B=1000) {

  # Generating the indices
  n <- length(y)
  indices <- sample.int(n = n, size = n * B, replace = TRUE) |>
    matrix(nrow = n)

  


  # Fitting the model
  apply(indices, 2, function(i) {
    glm(y[i] ~ X[i,], family = binomial("logit")) |>
      coef()
  }) |> t()

} 


set.seed(3312)
ans <- my_boot(y, X, B=50)
head(ans)
```

## Ex 3: Bootstrap - Parallel

```{r}
#| label: parallel-ex-bootstrap-parallel
#| echo: true
my_boot_pll <- function(y, X, cl, B=1000) {

  # Generating the indices
  n <- length(y)
  indices <- sample.int(n = n, size = n * B, replace = TRUE) |>
    matrix(nrow = n)

  # Making sure y and X are available in the cluster
  parallel::clusterExport(cl, c("y", "X"))

  # Fitting the model
  parallel::parApply(cl, indices, 2, function(i) {
    glm(y[i] ~ X[i,], family = binomial("logit")) |>
      coef()
  }) |> t()

}

cl <- parallel::makeForkCluster(4)
set.seed(3312)
ans_pll <- my_boot_pll(y, X, cl, B=50)
head(ans_pll)
```

---

How much faster?

```{r}
#| label: parallel-ex-bootstrap-bench
#| echo: true
#| warning: false
#| cache: true
microbenchmark::microbenchmark(
  parallel = my_boot_pll(y, X, cl, B=1000),
  serial   = my_boot(y, X, B=1000),
  times    = 1,
  unit     = "s"
)
parallel::stopCluster(cl)
```


## Ex 4: Overhead cost

Problem: Revisit of the overhead cost of parallelization. We want to fit the following model $$y = X_k\beta_k + \varepsilon,\quad k = 1, \dots$$

```{r}
#| label: overhead-cost-data
# Simulating some data
n <- 1e4
k <- 3e3
X <- matrix(rnorm(n*k), ncol=k)
y <- rnorm(n)

X[1:4, 1:5]
y[1:6]
```

## Ex 4: Overhead cost (cont.)

- Naive approach: run regressions and return the **full** output.

- Problem: The `lm()` function returns a lot of information. Recovering all that information is costly:

```{r}
#| echo: true
#| label: overhead-cost-lm
lm(y ~ X[,1]) |> str()
```

## Ex 4: Overhead cost - Implementation

Let's compare the cost of getting only the coefficients and minimizing copy via forking:

```{r}
#| label: overhead-cost-run
#| echo: true
library(parallel)
cl <- makePSOCKcluster(4)

# Running the benchmark
cost_serial <- system.time(apply(X, 2, function(x, y) lm(y ~ x), y = y))
cost_pll <- system.time(parApply(cl, X, 2, function(x, y) lm(y ~ x), y = y))
cost_pll_coef <- system.time(parApply(cl, X, 2, function(x, y) coef(lm(y ~ x)), y = y))

# Stopping the cluster
stopCluster(cl)

cost_pll_fork <- system.time({
  mclapply(1:ncol(X), function(j) coef(lm(y ~ X[,j])), mc.cores = 4)
})
```

```{r}
#| label: overhead-cost-stop
data.frame(
  Type = c(
    "Serial", "Parallel", "Parallel (coef only)",
    "Parallel fork (coef only)"
    ),
  Elapsed = c(
    cost_serial[3], cost_pll[3], cost_pll_coef[3],
    cost_pll_fork[3]
    )
) |> t() |> knitr::kable()
```

## {style="text-align:center!important;"}

```{r thanks, out.width="300px", echo=FALSE}
knitr::include_graphics("fig/speed.gif")
```

### Thanks!

<p style="text-align:center!important;">
{{< fa brands github >}}   [gvegayon](https://github.com/gvegayon/) <br>
{{< fa home >}} [ggvy.cl](https://ggvy.cl)<br>
{{< fa envelope >}} [george.vegayon@utah.edu](mailto:george.vegayon@utah.edu) <br>
<text style="color:gray;font-size:80%">Presentation created with {{< fa heart >}} and [quarto.org](https://quarto.org)</text>
</p>

## See also

*   [Package parallel](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf) 
*   [Using the iterators package](https://cran.r-project.org/web/packages/iterators/vignettes/iterators.pdf)
*   [Using the foreach package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf)
*   [32 OpenMP traps for C++ developers](https://software.intel.com/en-us/articles/32-openmp-traps-for-c-developers)
*   [The OpenMP API specification for parallel programming](http://www.openmp.org/)
*   ['openmp' tag in Rcpp gallery](gallery.rcpp.org/tags/openmp/)
*   [OpenMP tutorials and articles](http://www.openmp.org/resources/tutorials-articles/)

For more, checkout the [CRAN Task View on HPC](https://cran.r-project.org/web/views/HighPerformanceComputing.html){target="_blank"}

## Bonus track: Simulating $\pi$ 


*   We know that $\pi = \frac{A}{r^2}$. We approximate it by randomly adding
    points $x$ to a square of size 2 centered at the origin.

*   So, we approximate $\pi$ as $\Pr\{\|x\| \leq 1\}\times 2^2$

```{r, echo=FALSE, dev='jpeg', dev.args=list(quality=100), fig.width=6, fig.height=6, out.width='300px', out.height='300px'}
set.seed(1231)
p    <- matrix(runif(5e3*2, -1, 1), ncol=2)
pcol <- ifelse(sqrt(rowSums(p^2)) <= 1, adjustcolor("blue", .7), adjustcolor("gray", .7))
plot(p, col=pcol, pch=18)
```

## 

The R code to do this

```{r simpi, echo=TRUE}
pisim <- function(i, nsim) {  # Notice we don't use the -i-
  # Random points
  ans  <- matrix(runif(nsim*2), ncol=2)
  
  # Distance to the origin
  ans  <- sqrt(rowSums(ans^2))
  
  # Estimated pi
  (sum(ans <= 1)*4)/nsim
}
```

##

```{r parallel-ex2, echo=TRUE, cache=TRUE}
library(parallel)
# Setup
cl <- makePSOCKcluster(4L)
clusterSetRNGStream(cl, 123)

# Number of simulations we want each time to run
nsim <- 1e5

# We need to make -nsim- and -pisim- available to the
# cluster
clusterExport(cl, c("nsim", "pisim"))

# Benchmarking: parSapply and sapply will run this simulation
# a hundred times each, so at the end we have 1e5*100 points
# to approximate pi
microbenchmark::microbenchmark(
  parallel = parSapply(cl, 1:100, pisim, nsim=nsim),
  serial   = sapply(1:100, pisim, nsim=nsim),
  times    = 10,
  unit     = "relative"
)

```


## Session info

```{r session, echo=FALSE}
sessionInfo()
```

## References
